{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "TODO: OBIETTIVI DEL PROGETTO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: ARCHITETTURA E TECNOLOGIE UTILIZZATE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RISULTATI:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.0.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.7.3 (default, Mar  6 2020 22:34:30)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "from IPython.core.display import display\n",
    "from pyspark.shell import spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Reading all the 10 years CSV\n",
    "year2005 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2005_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2006 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2006_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2007 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2007_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2008 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2008_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2009 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2009_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2010 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2010_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2011 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2011_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2012 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2012_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2013 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2013_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2014 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2014_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2015 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2015_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all 11 years data into dataframe\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "MergeData = unionAll(year2005,\n",
    "                     year2006,\n",
    "                     year2007,\n",
    "                     year2008,\n",
    "                     year2009,\n",
    "                     year2010,\n",
    "                     year2011,\n",
    "                     year2012,\n",
    "                     year2013,\n",
    "                     year2014,\n",
    "                     year2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Count of Merged Data\n",
    "#MergeData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#Count of Merged Data\n",
    "#MergeData.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Create SQL Table from data frame\n",
    "MergeData.registerTempTable(\"mergedTable\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+-----------+\n",
      "|resident_status|sex|sex_counter|\n",
      "+---------------+---+-----------+\n",
      "|              1|  F|   11419032|\n",
      "|              1|  M|   11030268|\n",
      "|              2|  F|    2027162|\n",
      "|              2|  M|    2288859|\n",
      "|              3|  F|     398187|\n",
      "|              3|  M|     506845|\n",
      "|              4|  F|      16495|\n",
      "|              4|  M|      33825|\n",
      "+---------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Male vs female deaths by resident status\n",
    "results_male_female_resident_status = spark.sql(\n",
    "  \"\"\"SELECT resident_status,\n",
    "            sex,\n",
    "            count(sex) AS sex_counter\n",
    "     FROM mergedTable\n",
    "     GROUP BY resident_status, sex\n",
    "     ORDER BY resident_status, sex\"\"\")\n",
    "\n",
    "results_male_female_resident_status.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['127.0.0.1'], port= 9042)\n",
    "session = cluster.connect('mykeyspace')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "stmt = session.prepare(\"INSERT INTO results_male_female_resident_status (i, resident_status, sex, sex_counter) VALUES (?, ?, ?, ?)\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for i, item in results_male_female_resident_status.toPandas().iterrows():\n",
    "    #values = '{},{},{}'.format(i, item['resident_status'], item['sex'])\n",
    "    #session.execute(\"INSERT INTO results_male_female_resident_status (i, resident_status, sex) VALUES ({})\".format(values))\n",
    "    results = session.execute(stmt, [i, item['resident_status'], item['sex'], item['sex_counter']])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----------+\n",
      "|month_of_death|sex|count(sex)|\n",
      "+--------------+---+----------+\n",
      "|             1|  F|   1302416|\n",
      "|             1|  M|   1269199|\n",
      "|             2|  F|   1177388|\n",
      "|             2|  M|   1147291|\n",
      "|             3|  F|   1262972|\n",
      "|             3|  M|   1231322|\n",
      "|             4|  F|   1150376|\n",
      "|             4|  M|   1145743|\n",
      "|             5|  F|   1134538|\n",
      "|             5|  M|   1144327|\n",
      "|             6|  F|   1066368|\n",
      "|             6|  M|   1087923|\n",
      "|             7|  F|   1088399|\n",
      "|             7|  M|   1116795|\n",
      "|             8|  F|   1083513|\n",
      "|             8|  M|   1107852|\n",
      "|             9|  F|   1065444|\n",
      "|             9|  M|   1081367|\n",
      "|            10|  F|   1144472|\n",
      "|            10|  M|   1147902|\n",
      "+--------------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - Male vs female deaths by month of the year\n",
    "results_male_deaths_month = spark.sql(\n",
    "  \"\"\"SELECT month_of_death,\n",
    "            sex,\n",
    "            count(sex)\n",
    "     FROM mergedTable\n",
    "     GROUP BY month_of_death, sex\n",
    "     ORDER BY month_of_death, sex\"\"\")\n",
    "\n",
    "results_male_deaths_month.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Read Disease description CSV (ICD10)\n",
    "icd10 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/codes/ICD10.csv',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        ignoreLeadingWhiteSpace=True,\n",
    "                        ignoreTrailingWhiteSpace=True).cache())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#Create SQL Table of disease description\n",
    "icd10.registerTempTable(\"icd10\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+--------------------+----------+\n",
      "|sex|icd_code_10th_revision|         description|count(sex)|\n",
      "+---+----------------------+--------------------+----------+\n",
      "|  M|                  I219|Acute myocardial ...|    765648|\n",
      "|  F|                  G309| Alzheimer's disease|    626180|\n",
      "|  F|                  I219|Acute myocardial ...|    622679|\n",
      "|  F|                  J449|Other chronic obs...|    602655|\n",
      "|  M|                  J449|Other chronic obs...|    534241|\n",
      "|  M|                   C61|Malignant neoplas...|    311877|\n",
      "|  F|                  J189|Pneumonia, unspec...|    276400|\n",
      "|  M|                  G309| Alzheimer's disease|    270020|\n",
      "|  M|                  J189|Pneumonia, unspec...|    236009|\n",
      "|  M|                  C189|Malignant neoplas...|    228669|\n",
      "|  F|                  C189|Malignant neoplas...|    223213|\n",
      "|  M|                  C259|Malignant neoplas...|    203498|\n",
      "|  F|                  C259|Malignant neoplas...|    199666|\n",
      "|  F|                  A419|Sepsis, unspecifi...|    199493|\n",
      "|  M|                  A419|Sepsis, unspecifi...|    169382|\n",
      "|  M|                   G20| Parkinson's disease|    145311|\n",
      "|  M|                  I119|Hypertensive hear...|    138554|\n",
      "|  M|                  C159|Malignant neoplas...|    123748|\n",
      "|  F|                   I10|Essential (primar...|    116720|\n",
      "|  M|                  C679|Malignant neoplas...|    114029|\n",
      "+---+----------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Top 20 diseases causing deaths for either sex\n",
    "results_diseases_causing_deaths_sex = spark.sql(\n",
    "  \"\"\"SELECT m.sex,\n",
    "            m.icd_code_10th_revision,\n",
    "            i.description3 AS description,\n",
    "            count(m.sex)\n",
    "     FROM mergedTable m, icd10 i\n",
    "     WHERE i.code3 = m.icd_code_10th_revision\n",
    "     GROUP BY i.description3 ,m.icd_code_10th_revision, m.sex\n",
    "     ORDER BY count(m.sex) DESC, m.sex\n",
    "     LIMIT 20\"\"\")\n",
    "\n",
    "results_diseases_causing_deaths_sex.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------+\n",
      "|Year|MethodofDisposition|  Count|\n",
      "+----+-------------------+-------+\n",
      "|2005|              Other|   2199|\n",
      "|2005|           Donation|   4795|\n",
      "|2005|         Entombment|  21247|\n",
      "|2005|     RemovedFromUSA|  31954|\n",
      "|2005|          Cremation| 350018|\n",
      "|2005|             Burial| 553202|\n",
      "|2005|            Unknown|1489091|\n",
      "|2006|              Other|   2252|\n",
      "|2006|           Donation|   6883|\n",
      "|2006|         Entombment|  23412|\n",
      "|2006|     RemovedFromUSA|  40870|\n",
      "|2006|          Cremation| 423282|\n",
      "|2006|             Burial| 667169|\n",
      "|2006|            Unknown|1266857|\n",
      "|2007|              Other|   3119|\n",
      "|2007|           Donation|   8719|\n",
      "|2007|         Entombment|  26139|\n",
      "|2007|     RemovedFromUSA|  41411|\n",
      "|2007|          Cremation| 472220|\n",
      "|2007|             Burial| 725666|\n",
      "+----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - Method of Disposition\n",
    "results_disposition = spark.sql(\n",
    "  \"\"\"SELECT current_data_year AS Year,\n",
    "            CASE method_of_disposition\n",
    "            WHEN 'C' THEN 'Cremation'\n",
    "            WHEN 'B' THEN 'Burial'\n",
    "            WHEN 'D'THEN 'Donation'\n",
    "            WHEN 'E' THEN 'Entombment'\n",
    "            WHEN 'O' THEN 'Other'\n",
    "            WHEN 'R' THEN 'RemovedFromUSA'\n",
    "            WHEN 'U' THEN 'Unknown'\n",
    "            END AS MethodofDisposition,\n",
    "            COUNT(*) AS Count\n",
    "     FROM mergedTable\n",
    "     GROUP BY 1, 2\n",
    "     ORDER BY 1, 3\"\"\")\n",
    "\n",
    "results_disposition.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 39.0 failed 1 times, most recent failure: Lost task 16.0 in stage 39.0 (TID 1509, 192.168.31.215, executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 33051\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:794)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:200)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:207)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.<init>(UnsafeFixedWidthAggregationMap.java:101)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:434)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1076)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1498)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1486)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 33051\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:794)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:200)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:207)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.<init>(UnsafeFixedWidthAggregationMap.java:101)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:434)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-ea2cc93a7fa6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m      ORDER BY 1,2\"\"\")\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m \u001B[0mresults_deaths_month\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Projects/usa-mortality-analysis/venv/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mshow\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    438\u001B[0m         \"\"\"\n\u001B[1;32m    439\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mtruncate\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 440\u001B[0;31m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    441\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    442\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshowString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtruncate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvertical\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Projects/usa-mortality-analysis/venv/lib/python3.7/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Projects/usa-mortality-analysis/venv/lib/python3.7/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    129\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 131\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    132\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/Projects/usa-mortality-analysis/venv/lib/python3.7/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o157.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 39.0 failed 1 times, most recent failure: Lost task 16.0 in stage 39.0 (TID 1509, 192.168.31.215, executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 33051\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:794)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:200)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:207)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.<init>(UnsafeFixedWidthAggregationMap.java:101)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:434)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2188)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1094)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1076)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1498)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1486)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 262144 bytes of memory, got 33051\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.allocate(BytesToBytesMap.java:794)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:200)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap.<init>(BytesToBytesMap.java:207)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.<init>(UnsafeFixedWidthAggregationMap.java:101)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.createHashMap(HashAggregateExec.scala:434)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# 5 - Manner of death per month\n",
    "results_deaths_month = spark.sql(\n",
    "  \"\"\"SELECT month_of_death AS Month,\n",
    "            CASE manner_of_death\n",
    "            WHEN '0' THEN 'Not Specified'\n",
    "            WHEN '1' THEN 'Accident'\n",
    "            WHEN '2' THEN 'Suicide'\n",
    "            WHEN '3' THEN 'Homicide'\n",
    "            WHEN '4' THEN 'Pending investigation'\n",
    "            WHEN '5' THEN 'Could not be determine'\n",
    "            WHEN '6' THEN 'Self-Inflicted'\n",
    "            WHEN '7' THEN 'Natural'\n",
    "            ELSE 'OTHER'\n",
    "            END AS MannerOfDeath,\n",
    "            COUNT(*) AS Count\n",
    "     FROM mergedTable\n",
    "     GROUP BY 1, 2\n",
    "     ORDER BY 1,2\"\"\")\n",
    "\n",
    "results_deaths_month.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6 - Analysis about Alzheimer's death order by group age\n",
    "results_alzheimer = spark.sql(\n",
    "  \"\"\"SELECT count(*) as TotalDeaths,\n",
    "            CASE age_recode_12\n",
    "            WHEN '10' THEN '75 - 84 years'\n",
    "            WHEN '11' THEN '85 years and over'\n",
    "            WHEN '12' THEN 'Age not stated'\n",
    "            WHEN '01' THEN 'Under 1 year'\n",
    "            WHEN '02' THEN '1 - 4 years'\n",
    "            WHEN '03' THEN '5 - 14 years'\n",
    "            WHEN '04' THEN '15 - 24 years'\n",
    "            WHEN '05' THEN '25 - 34 years'\n",
    "            WHEN '06' THEN '35 - 44 years'\n",
    "            WHEN '07' THEN '45 - 54 years'\n",
    "            WHEN '08' THEN '55 - 64 years'\n",
    "            WHEN '09' THEN '65 - 74 years'\n",
    "            END AS age\n",
    "     FROM mergedTable\n",
    "     WHERE 113_cause_recode = '052'\n",
    "     GROUP BY age\n",
    "     ORDER BY TotalDeaths\n",
    "     DESC limit 10\"\"\")\n",
    "\n",
    "results_alzheimer.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7 - Count number of graduate people who deads with suicide manner\n",
    "results_degree_suicides = spark.sql(\n",
    "  \"\"\"SELECT count(*) as TotalDeaths,\n",
    "            CASE education_2003_revision\n",
    "            WHEN '1' THEN '8th grade or less'\n",
    "            WHEN '2' THEN '9 - 12th grade, no diploma'\n",
    "            WHEN '3' THEN 'high school graduate or GED completed'\n",
    "            WHEN '4' THEN 'some college credit, but no degree'\n",
    "            WHEN '5' THEN 'Associate degree'\n",
    "            WHEN '6' THEN 'Bachelor’s degree'\n",
    "            WHEN '7' THEN 'Master’s degree'\n",
    "            WHEN '8' THEN 'Doctorate or professional degree'\n",
    "            WHEN '9' THEN 'Unknown'\n",
    "            END AS studies\n",
    "     FROM mergedTable\n",
    "     WHERE manner_of_death = '2'\n",
    "     GROUP BY studies\n",
    "     ORDER BY TotalDeaths\n",
    "     DESC limit 10\"\"\")\n",
    "\n",
    "results_degree_suicides.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Columns of Merge Data\n",
    "MergeData.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO: DATA PRE-PROCESSING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dropping Columns\n",
    "MergeData = MergeData.drop('record_condition_1',\n",
    "                           'record_condition_2',\n",
    "                           'record_condition_3',\n",
    "                           'record_condition_4',\n",
    "                           'record_condition_5',\n",
    "                           'record_condition_6',\n",
    "                           'record_condition_7',\n",
    "                           'record_condition_8',\n",
    "                           'record_condition_9',\n",
    "                           'record_condition_10',\n",
    "                           'record_condition_11',\n",
    "                           'record_condition_12',\n",
    "                           'record_condition_13',\n",
    "                           'record_condition_14',\n",
    "                           'record_condition_15',\n",
    "                           'record_condition_16',\n",
    "                           'record_condition_17',\n",
    "                           'record_condition_18',\n",
    "                           'record_condition_19',\n",
    "                           'record_condition_20')\n",
    "\n",
    "MergeData = MergeData.drop('113_cause_recode',\n",
    "                           '130_infant_cause_recode',\n",
    "                           '39_cause_recode',\n",
    "                           'number_of_entity_axis_conditions',\n",
    "                           'entity_condition_1',\n",
    "                           'entity_condition_2',\n",
    "                           'entity_condition_3',\n",
    "                           'entity_condition_4',\n",
    "                           'entity_condition_5',\n",
    "                           'entity_condition_6',\n",
    "                           'entity_condition_7',\n",
    "                           'entity_condition_8',\n",
    "                           'entity_condition_9',\n",
    "                           'entity_condition_10',\n",
    "                           'entity_condition_11',\n",
    "                           'entity_condition_12',\n",
    "                           'entity_condition_13',\n",
    "                           'entity_condition_14',\n",
    "                           'entity_condition_15',\n",
    "                           'entity_condition_16',\n",
    "                           'entity_condition_17',\n",
    "                           'entity_condition_18',\n",
    "                           'entity_condition_19')\n",
    "\n",
    "MergeData = MergeData.drop('icd_code_10th_revision',\n",
    "                           'age_recode_27',\n",
    "                           'age_recode_12',\n",
    "                           'detail_age',\n",
    "                           'entity_condition_20',\n",
    "                           'education_2003_revision',\n",
    "                           'education_1989_revision')\n",
    "\n",
    "MergeData = MergeData.filter((MergeData.method_of_disposition == 'B') | (MergeData.method_of_disposition == 'C' ))\n",
    "\n",
    "MergeData = MergeData.drop('detail_age_type',\n",
    "                           'age_substitution_flag',\n",
    "                           'age_substitution_flag',\n",
    "                           'infant_age_recode_22',\n",
    "                           'day_of_week_of_death',\n",
    "                           'current_data_year',\n",
    "                           '358_cause_recode',\n",
    "                           'number_of_record_axis_conditions',\n",
    "                           'hispanic_origin',\n",
    "                           'race_recode_5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Columns of Merge Data\n",
    "MergeData.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "categoricalColumns = ['resident_status',\n",
    "                      'month_of_death',\n",
    "                      'sex',\n",
    "                      'age_recode_52',\n",
    "                      'place_of_death_and_decedents_status',\n",
    "                      'marital_status',\n",
    "                      'injury_at_work',\n",
    "                      'manner_of_death',\n",
    "                      'autopsy',\n",
    "                      'activity_code',\n",
    "                      'place_of_injury_for_causes_w00_y34_except_y06_and_y07_',\n",
    "                      'race',\n",
    "                      'race_recode_3',\n",
    "                      'hispanic_originrace_recode']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Null Imputations\n",
    "MergeData = MergeData.fillna({'place_of_injury_for_causes_w00_y34_except_y06_and_y07_': 12})\n",
    "MergeData = MergeData.fillna({'activity_code': 11})\n",
    "MergeData = MergeData.fillna({'manner_of_death': 999})\n",
    "MergeData = MergeData.fillna({'place_of_death_and_decedents_status': 999})\n",
    "\n",
    "# End Data pre-processing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merging all 11 years data into dataframe\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "JoinedData=unionAll(year2005,\n",
    "                    year2006,\n",
    "                    year2007,\n",
    "                    year2008,\n",
    "                    year2009,\n",
    "                    year2010,\n",
    "                    year2011,\n",
    "                    year2012,\n",
    "                    year2013,\n",
    "                    year2014,\n",
    "                    year2015)\n",
    "\n",
    "display(JoinedData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Replacing null values in manner_of_death\n",
    "JoinData = JoinedData.fillna({'manner_of_death': 12})\n",
    "JoinData = JoinedData.filter(JoinedData.manner_of_death == '2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Creating joint table on new joined data which is modified\n",
    "JoinData.registerTempTable(\"jointTable\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8 - Total suicides committed 2005-2015\n",
    "results_suicides = spark.sql(\n",
    "  \"\"\"SELECT current_data_year AS Year,\n",
    "            CASE manner_of_death\n",
    "            WHEN '2' THEN 'Suicide'\n",
    "            END AS TotalSuicidesCommited,\n",
    "            COUNT(*) AS Count\n",
    "     FROM jointTable\n",
    "     GROUP BY 1, 2\n",
    "     ORDER BY 1, 3\"\"\")\n",
    "\n",
    "results_suicides.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 9 - Result about deaths pending\n",
    "results_deaths_pending = spark.sql(\n",
    "    \"\"\"SELECT icd.description3, count(*) as TotalDeaths\n",
    "       FROM mergedTable mt\n",
    "       JOIN icd10 icd ON icd.code3 = mt.icd_code_10th_revision\n",
    "       WHERE mt.manner_of_death == '4'\n",
    "       GROUP BY icd.description3\n",
    "       ORDER BY COUNT(*) DESC, icd.description3\"\"\"\n",
    ")\n",
    "results_deaths_pending.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 10 - Deaths for days of the week\n",
    "results_day_week = spark.sql(\n",
    "    \"\"\"SELECT count(*) as TotalDeaths,\n",
    "              CASE day_of_week_of_death\n",
    "              WHEN '1' THEN 'Sunday'\n",
    "              WHEN '2' THEN 'Monday'\n",
    "              WHEN '3' THEN 'Tuesday'\n",
    "              WHEN '4' THEN 'Wednesday'\n",
    "              WHEN '5' THEN 'Thursday'\n",
    "              WHEN '6' THEN 'Friday'\n",
    "              WHEN '7' THEN 'Saturday'\n",
    "              WHEN '9' THEN 'Unknown' END AS day\n",
    "      FROM mergedTable\n",
    "      GROUP BY day\n",
    "      ORDER BY TotalDeaths\"\"\"\n",
    ")\n",
    "\n",
    "results_day_week.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 11 - Total deaths by skin's color\n",
    "results_deaths_skin_color = spark.sql(\n",
    "    \"\"\"SELECT count(*) as TotalDeaths,\n",
    "            CASE race_recode_3\n",
    "            WHEN '1' THEN 'White'\n",
    "            WHEN '2' THEN 'Races other than b&w'\n",
    "            WHEN '3' THEN 'Black' END AS race\n",
    "       FROM mergedTable\n",
    "       GROUP BY race_recode_3\n",
    "       ORDER BY TotalDeaths\"\"\"\n",
    ")\n",
    "\n",
    "results_deaths_skin_color.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}