{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "from IPython.core.display import display\n",
    "from pyspark.shell import spark\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import reduce  # For Python 3.x\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Spark configuration\n",
    "sparkConf=SparkConf().setMaster(\"local[3]\")\\\n",
    "                     .setAppName(\"project\")\\\n",
    "                     .setAll([('spark.executor.memory','2g'),\n",
    "                            ('spark.sql.debug.maxToStringFields','1000'),\n",
    "                            ('spark.driver.memory','2g')])\n",
    "\n",
    "spark=SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Connection with Cassandra db\n",
    "cluster = Cluster(['127.0.0.1'], port= 9042)\n",
    "session = cluster.connect('mykeyspace')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the 10 years CSV\n",
    "year2005 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2005_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2006 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2006_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2007 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2007_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2008 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2008_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2009 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2009_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2010 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2010_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2011 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2011_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2012 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2012_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2013 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2013_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2014 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2014_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())\n",
    "year2015 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/dataset/2015_data.csv',\n",
    "                           header=True,\n",
    "                           inferSchema=True,\n",
    "                           ignoreLeadingWhiteSpace=True,\n",
    "                           ignoreTrailingWhiteSpace=True).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all 11 years data into dataframe\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "MergeData = unionAll(year2005,\n",
    "                     year2006,\n",
    "                     year2007,\n",
    "                     year2008,\n",
    "                     year2009,\n",
    "                     year2010,\n",
    "                     year2011,\n",
    "                     year2012,\n",
    "                     year2013,\n",
    "                     year2014,\n",
    "                     year2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "27720673"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of MergeData\n",
    "MergeData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- resident_status: integer (nullable = true)\n",
      " |-- education_1989_revision: integer (nullable = true)\n",
      " |-- education_2003_revision: integer (nullable = true)\n",
      " |-- education_reporting_flag: integer (nullable = true)\n",
      " |-- month_of_death: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- detail_age_type: integer (nullable = true)\n",
      " |-- detail_age: integer (nullable = true)\n",
      " |-- age_substitution_flag: integer (nullable = true)\n",
      " |-- age_recode_52: integer (nullable = true)\n",
      " |-- age_recode_27: integer (nullable = true)\n",
      " |-- age_recode_12: integer (nullable = true)\n",
      " |-- infant_age_recode_22: integer (nullable = true)\n",
      " |-- place_of_death_and_decedents_status: integer (nullable = true)\n",
      " |-- marital_status: string (nullable = true)\n",
      " |-- day_of_week_of_death: integer (nullable = true)\n",
      " |-- current_data_year: integer (nullable = true)\n",
      " |-- injury_at_work: string (nullable = true)\n",
      " |-- manner_of_death: integer (nullable = true)\n",
      " |-- method_of_disposition: string (nullable = true)\n",
      " |-- autopsy: string (nullable = true)\n",
      " |-- activity_code: integer (nullable = true)\n",
      " |-- place_of_injury_for_causes_w00_y34_except_y06_and_y07_: integer (nullable = true)\n",
      " |-- icd_code_10th_revision: string (nullable = true)\n",
      " |-- 358_cause_recode: integer (nullable = true)\n",
      " |-- 113_cause_recode: integer (nullable = true)\n",
      " |-- 130_infant_cause_recode: integer (nullable = true)\n",
      " |-- 39_cause_recode: integer (nullable = true)\n",
      " |-- number_of_entity_axis_conditions: integer (nullable = true)\n",
      " |-- entity_condition_1: string (nullable = true)\n",
      " |-- entity_condition_2: string (nullable = true)\n",
      " |-- entity_condition_3: string (nullable = true)\n",
      " |-- entity_condition_4: string (nullable = true)\n",
      " |-- entity_condition_5: string (nullable = true)\n",
      " |-- entity_condition_6: string (nullable = true)\n",
      " |-- entity_condition_7: string (nullable = true)\n",
      " |-- entity_condition_8: string (nullable = true)\n",
      " |-- entity_condition_9: string (nullable = true)\n",
      " |-- entity_condition_10: string (nullable = true)\n",
      " |-- entity_condition_11: string (nullable = true)\n",
      " |-- entity_condition_12: string (nullable = true)\n",
      " |-- entity_condition_13: string (nullable = true)\n",
      " |-- entity_condition_14: string (nullable = true)\n",
      " |-- entity_condition_15: string (nullable = true)\n",
      " |-- entity_condition_16: string (nullable = true)\n",
      " |-- entity_condition_17: string (nullable = true)\n",
      " |-- entity_condition_18: string (nullable = true)\n",
      " |-- entity_condition_19: string (nullable = true)\n",
      " |-- entity_condition_20: string (nullable = true)\n",
      " |-- number_of_record_axis_conditions: integer (nullable = true)\n",
      " |-- record_condition_1: string (nullable = true)\n",
      " |-- record_condition_2: string (nullable = true)\n",
      " |-- record_condition_3: string (nullable = true)\n",
      " |-- record_condition_4: string (nullable = true)\n",
      " |-- record_condition_5: string (nullable = true)\n",
      " |-- record_condition_6: string (nullable = true)\n",
      " |-- record_condition_7: string (nullable = true)\n",
      " |-- record_condition_8: string (nullable = true)\n",
      " |-- record_condition_9: string (nullable = true)\n",
      " |-- record_condition_10: string (nullable = true)\n",
      " |-- record_condition_11: string (nullable = true)\n",
      " |-- record_condition_12: string (nullable = true)\n",
      " |-- record_condition_13: string (nullable = true)\n",
      " |-- record_condition_14: string (nullable = true)\n",
      " |-- record_condition_15: string (nullable = true)\n",
      " |-- record_condition_16: string (nullable = true)\n",
      " |-- record_condition_17: string (nullable = true)\n",
      " |-- record_condition_18: string (nullable = true)\n",
      " |-- record_condition_19: string (nullable = true)\n",
      " |-- record_condition_20: string (nullable = true)\n",
      " |-- race: integer (nullable = true)\n",
      " |-- bridged_race_flag: integer (nullable = true)\n",
      " |-- race_imputation_flag: integer (nullable = true)\n",
      " |-- race_recode_3: integer (nullable = true)\n",
      " |-- race_recode_5: integer (nullable = true)\n",
      " |-- hispanic_origin: integer (nullable = true)\n",
      " |-- hispanic_originrace_recode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show MergeData schema\n",
    "MergeData.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create SQL Table from MergeData\n",
    "MergeData.registerTempTable(\"mergedTable\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+----------+\n",
      "|resident_status|sex|sex_counts|\n",
      "+---------------+---+----------+\n",
      "|              1|  F|  11419032|\n",
      "|              1|  M|  11030268|\n",
      "|              2|  F|   2027162|\n",
      "|              2|  M|   2288859|\n",
      "|              3|  F|    398187|\n",
      "|              3|  M|    506845|\n",
      "|              4|  F|     16495|\n",
      "|              4|  M|     33825|\n",
      "+---------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1 - Male vs female deaths by resident status\n",
    "results_male_female_resident_status = spark.sql(\n",
    "  \"\"\"SELECT resident_status,\n",
    "            sex,\n",
    "            count(sex) AS sex_counts\n",
    "     FROM mergedTable\n",
    "     GROUP BY resident_status, sex\n",
    "     ORDER BY resident_status, sex\"\"\")\n",
    "\n",
    "results_male_female_resident_status.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_male_female_resident_status (ind int primary key, resident_status int, sex varchar, sex_counts int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_male_female_resident_status (ind, resident_status, sex, sex_counts) VALUES (?, ?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_male_female_resident_status.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['resident_status'], item['sex'], item['sex_counts']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+----------+\n",
      "|month_of_death|sex|sex_counts|\n",
      "+--------------+---+----------+\n",
      "|             1|  F|   1302416|\n",
      "|             1|  M|   1269199|\n",
      "|             2|  F|   1177388|\n",
      "|             2|  M|   1147291|\n",
      "|             3|  F|   1262972|\n",
      "|             3|  M|   1231322|\n",
      "|             4|  F|   1150376|\n",
      "|             4|  M|   1145743|\n",
      "|             5|  F|   1134538|\n",
      "|             5|  M|   1144327|\n",
      "|             6|  F|   1066368|\n",
      "|             6|  M|   1087923|\n",
      "|             7|  F|   1088399|\n",
      "|             7|  M|   1116795|\n",
      "|             8|  F|   1083513|\n",
      "|             8|  M|   1107852|\n",
      "|             9|  F|   1065444|\n",
      "|             9|  M|   1081367|\n",
      "|            10|  F|   1144472|\n",
      "|            10|  M|   1147902|\n",
      "+--------------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2 - Male vs female deaths by month of the year\n",
    "results_male_deaths_month = spark.sql(\n",
    "  \"\"\"SELECT month_of_death,\n",
    "            sex,\n",
    "            count(sex) AS sex_counts\n",
    "     FROM mergedTable\n",
    "     GROUP BY month_of_death, sex\n",
    "     ORDER BY month_of_death, sex\"\"\")\n",
    "\n",
    "results_male_deaths_month.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_male_deaths_month (ind int primary key, month_of_death int, sex varchar, sex_counts int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_male_deaths_month (ind, month_of_death, sex, sex_counts) VALUES (?, ?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_male_deaths_month.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['month_of_death'], item['sex'], item['sex_counts']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Read Disease description CSV (ICD10)\n",
    "icd10 = (spark.read.csv(path='/Users/alessio/Documents/Projects/usa-mortality-analysis/sources/codes/ICD10.csv',\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        ignoreLeadingWhiteSpace=True,\n",
    "                        ignoreTrailingWhiteSpace=True).cache())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Create SQL Table of disease description\n",
    "icd10.registerTempTable(\"icd10\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+----------+\n",
      "|sex|code|         description|sex_counts|\n",
      "+---+----+--------------------+----------+\n",
      "|  M|I219|Acute myocardial ...|    765648|\n",
      "|  F|G309| Alzheimer's disease|    626180|\n",
      "|  F|I219|Acute myocardial ...|    622679|\n",
      "|  F|J449|Other chronic obs...|    602655|\n",
      "|  M|J449|Other chronic obs...|    534241|\n",
      "|  M| C61|Malignant neoplas...|    311877|\n",
      "|  F|J189|Pneumonia, unspec...|    276400|\n",
      "|  M|G309| Alzheimer's disease|    270020|\n",
      "|  M|J189|Pneumonia, unspec...|    236009|\n",
      "|  M|C189|Malignant neoplas...|    228669|\n",
      "|  F|C189|Malignant neoplas...|    223213|\n",
      "|  M|C259|Malignant neoplas...|    203498|\n",
      "|  F|C259|Malignant neoplas...|    199666|\n",
      "|  F|A419|Sepsis, unspecifi...|    199493|\n",
      "|  M|A419|Sepsis, unspecifi...|    169382|\n",
      "|  M| G20| Parkinson's disease|    145311|\n",
      "|  M|I119|Hypertensive hear...|    138554|\n",
      "|  M|C159|Malignant neoplas...|    123748|\n",
      "|  F| I10|Essential (primar...|    116720|\n",
      "|  M|C679|Malignant neoplas...|    114029|\n",
      "+---+----+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 3 - Top 20 diseases causing deaths for either sex\n",
    "results_diseases_causing_deaths_sex = spark.sql(\n",
    "  \"\"\"SELECT m.sex AS sex,\n",
    "            m.icd_code_10th_revision AS code,\n",
    "            i.description3 AS description,\n",
    "            count(m.sex) AS sex_counts\n",
    "     FROM mergedTable m, icd10 i\n",
    "     WHERE i.code3 = m.icd_code_10th_revision\n",
    "     GROUP BY i.description3 ,m.icd_code_10th_revision, m.sex\n",
    "     ORDER BY count(m.sex) DESC, m.sex\n",
    "     LIMIT 20\"\"\")\n",
    "\n",
    "results_diseases_causing_deaths_sex.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_diseases_causing_deaths_sex (ind int primary key, code varchar, description varchar, sex varchar, sex_counts int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_diseases_causing_deaths_sex (ind, code, description, sex, sex_counts) VALUES (?, ?, ?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_diseases_causing_deaths_sex.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['code'], item['description'], item['sex'], item['sex_counts']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-------+\n",
      "|year|   disposition| counts|\n",
      "+----+--------------+-------+\n",
      "|2005|         Other|   2199|\n",
      "|2005|      Donation|   4795|\n",
      "|2005|    Entombment|  21247|\n",
      "|2005|RemovedFromUSA|  31954|\n",
      "|2005|     Cremation| 350018|\n",
      "|2005|        Burial| 553202|\n",
      "|2005|       Unknown|1489091|\n",
      "|2006|         Other|   2252|\n",
      "|2006|      Donation|   6883|\n",
      "|2006|    Entombment|  23412|\n",
      "|2006|RemovedFromUSA|  40870|\n",
      "|2006|     Cremation| 423282|\n",
      "|2006|        Burial| 667169|\n",
      "|2006|       Unknown|1266857|\n",
      "|2007|         Other|   3119|\n",
      "|2007|      Donation|   8719|\n",
      "|2007|    Entombment|  26139|\n",
      "|2007|RemovedFromUSA|  41411|\n",
      "|2007|     Cremation| 472220|\n",
      "|2007|        Burial| 725666|\n",
      "+----+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 4 - Method of Disposition\n",
    "results_disposition = spark.sql(\n",
    "  \"\"\"SELECT current_data_year AS year,\n",
    "            CASE method_of_disposition\n",
    "            WHEN 'C' THEN 'Cremation'\n",
    "            WHEN 'B' THEN 'Burial'\n",
    "            WHEN 'D'THEN 'Donation'\n",
    "            WHEN 'E' THEN 'Entombment'\n",
    "            WHEN 'O' THEN 'Other'\n",
    "            WHEN 'R' THEN 'RemovedFromUSA'\n",
    "            WHEN 'U' THEN 'Unknown'\n",
    "            END AS disposition,\n",
    "            COUNT(*) AS counts\n",
    "     FROM mergedTable\n",
    "     GROUP BY 1, 2\n",
    "     ORDER BY 1, 3\"\"\")\n",
    "\n",
    "results_disposition.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_disposition (ind int primary key, counts int, disposition varchar, year int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_disposition (ind, counts, disposition, year) VALUES (?, ?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_disposition.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['counts'], item['disposition'], item['year']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+\n",
      "|month|        manner_death| counts|\n",
      "+-----+--------------------+-------+\n",
      "|    1|            Accident| 110563|\n",
      "|    1|Could not be dete...|  10808|\n",
      "|    1|            Homicide|  16172|\n",
      "|    1|             Natural|1859004|\n",
      "|    1|               OTHER| 536266|\n",
      "|    1|Pending investiga...|   4261|\n",
      "|    1|             Suicide|  34541|\n",
      "|    2|            Accident| 101932|\n",
      "|    2|Could not be dete...|   9667|\n",
      "|    2|            Homicide|  13474|\n",
      "|    2|             Natural|1677102|\n",
      "|    2|               OTHER| 487921|\n",
      "|    2|Pending investiga...|   3517|\n",
      "|    2|             Suicide|  31066|\n",
      "|    3|            Accident| 110882|\n",
      "|    3|Could not be dete...|  10421|\n",
      "|    3|            Homicide|  15866|\n",
      "|    3|             Natural|1799655|\n",
      "|    3|               OTHER| 517966|\n",
      "|    3|Pending investiga...|   3660|\n",
      "+-----+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 5 - Manner of death per month\n",
    "results_deaths_month = spark.sql(\n",
    "  \"\"\"SELECT month_of_death AS month,\n",
    "            CASE manner_of_death\n",
    "            WHEN '0' THEN 'Not Specified'\n",
    "            WHEN '1' THEN 'Accident'\n",
    "            WHEN '2' THEN 'Suicide'\n",
    "            WHEN '3' THEN 'Homicide'\n",
    "            WHEN '4' THEN 'Pending investigation'\n",
    "            WHEN '5' THEN 'Could not be determine'\n",
    "            WHEN '6' THEN 'Self-Inflicted'\n",
    "            WHEN '7' THEN 'Natural'\n",
    "            ELSE 'OTHER'\n",
    "            END AS manner_death,\n",
    "            COUNT(*) AS counts\n",
    "     FROM mergedTable\n",
    "     GROUP BY 1, 2\n",
    "     ORDER BY 1,2\"\"\")\n",
    "\n",
    "results_deaths_month.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_deaths_month (ind int primary key, counts int, manner_death varchar, month int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_deaths_month (ind, counts, manner_death, month) VALUES (?, ?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_deaths_month.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['counts'], item['manner_death'], item['month']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|deaths_counts|              age|\n",
      "+-------------+-----------------+\n",
      "|       595395|85 years and over|\n",
      "|       267194|    75 - 84 years|\n",
      "|        48876|    65 - 74 years|\n",
      "|         8598|    55 - 64 years|\n",
      "|         1082|    45 - 54 years|\n",
      "|          100|    35 - 44 years|\n",
      "|           11|   Age not stated|\n",
      "|            8|    25 - 34 years|\n",
      "|            1|    15 - 24 years|\n",
      "+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 6 - Analysis about Alzheimer's death order by group age\n",
    "results_alzheimer = spark.sql(\n",
    "  \"\"\"SELECT count(*) as deaths_counts,\n",
    "            CASE age_recode_12\n",
    "            WHEN '10' THEN '75 - 84 years'\n",
    "            WHEN '11' THEN '85 years and over'\n",
    "            WHEN '12' THEN 'Age not stated'\n",
    "            WHEN '01' THEN 'Under 1 year'\n",
    "            WHEN '02' THEN '1 - 4 years'\n",
    "            WHEN '03' THEN '5 - 14 years'\n",
    "            WHEN '04' THEN '15 - 24 years'\n",
    "            WHEN '05' THEN '25 - 34 years'\n",
    "            WHEN '06' THEN '35 - 44 years'\n",
    "            WHEN '07' THEN '45 - 54 years'\n",
    "            WHEN '08' THEN '55 - 64 years'\n",
    "            WHEN '09' THEN '65 - 74 years'\n",
    "            END AS age\n",
    "     FROM mergedTable\n",
    "     WHERE 113_cause_recode = '052'\n",
    "     GROUP BY age\n",
    "     ORDER BY deaths_counts DESC\n",
    "     LIMIT 10\"\"\")\n",
    "\n",
    "results_alzheimer.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_alzheimer (ind int primary key, age varchar, deaths_counts int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_alzheimer (ind, age, deaths_counts) VALUES (?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_alzheimer.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['age'], item['deaths_counts']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|deaths_counts|           education|\n",
      "+-------------+--------------------+\n",
      "|       120629|                null|\n",
      "|       117887|high school gradu...|\n",
      "|        51558|some college cred...|\n",
      "|        37228|9 - 12th grade, n...|\n",
      "|        35521|   Bachelor’s degree|\n",
      "|        20697|    Associate degree|\n",
      "|        12646|   8th grade or less|\n",
      "|        11644|     Master’s degree|\n",
      "|         9628|             Unknown|\n",
      "|         5923|Doctorate or prof...|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 7 - Count number of graduate people who deads with suicide manner\n",
    "results_degree_suicides = spark.sql(\n",
    "  \"\"\"SELECT count(*) as deaths_counts,\n",
    "            CASE education_2003_revision\n",
    "            WHEN '1' THEN '8th grade or less'\n",
    "            WHEN '2' THEN '9 - 12th grade, no diploma'\n",
    "            WHEN '3' THEN 'high school graduate or GED completed'\n",
    "            WHEN '4' THEN 'some college credit, but no degree'\n",
    "            WHEN '5' THEN 'Associate degree'\n",
    "            WHEN '6' THEN 'Bachelor’s degree'\n",
    "            WHEN '7' THEN 'Master’s degree'\n",
    "            WHEN '8' THEN 'Doctorate or professional degree'\n",
    "            WHEN '9' THEN 'Unknown'\n",
    "            END AS education\n",
    "     FROM mergedTable\n",
    "     WHERE manner_of_death = '2'\n",
    "     GROUP BY education\n",
    "     ORDER BY deaths_counts DESC\n",
    "     LIMIT 10\"\"\")\n",
    "\n",
    "results_degree_suicides.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_degree_suicides (ind int primary key, deaths_counts int, education varchar)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_degree_suicides (ind, deaths_counts, education) VALUES (?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_degree_suicides.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['deaths_counts'], item['education']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "['resident_status',\n 'education_1989_revision',\n 'education_2003_revision',\n 'education_reporting_flag',\n 'month_of_death',\n 'sex',\n 'detail_age_type',\n 'detail_age',\n 'age_substitution_flag',\n 'age_recode_52',\n 'age_recode_27',\n 'age_recode_12',\n 'infant_age_recode_22',\n 'place_of_death_and_decedents_status',\n 'marital_status',\n 'day_of_week_of_death',\n 'current_data_year',\n 'injury_at_work',\n 'manner_of_death',\n 'method_of_disposition',\n 'autopsy',\n 'activity_code',\n 'place_of_injury_for_causes_w00_y34_except_y06_and_y07_',\n 'icd_code_10th_revision',\n '358_cause_recode',\n '113_cause_recode',\n '130_infant_cause_recode',\n '39_cause_recode',\n 'number_of_entity_axis_conditions',\n 'entity_condition_1',\n 'entity_condition_2',\n 'entity_condition_3',\n 'entity_condition_4',\n 'entity_condition_5',\n 'entity_condition_6',\n 'entity_condition_7',\n 'entity_condition_8',\n 'entity_condition_9',\n 'entity_condition_10',\n 'entity_condition_11',\n 'entity_condition_12',\n 'entity_condition_13',\n 'entity_condition_14',\n 'entity_condition_15',\n 'entity_condition_16',\n 'entity_condition_17',\n 'entity_condition_18',\n 'entity_condition_19',\n 'entity_condition_20',\n 'number_of_record_axis_conditions',\n 'record_condition_1',\n 'record_condition_2',\n 'record_condition_3',\n 'record_condition_4',\n 'record_condition_5',\n 'record_condition_6',\n 'record_condition_7',\n 'record_condition_8',\n 'record_condition_9',\n 'record_condition_10',\n 'record_condition_11',\n 'record_condition_12',\n 'record_condition_13',\n 'record_condition_14',\n 'record_condition_15',\n 'record_condition_16',\n 'record_condition_17',\n 'record_condition_18',\n 'record_condition_19',\n 'record_condition_20',\n 'race',\n 'bridged_race_flag',\n 'race_imputation_flag',\n 'race_recode_3',\n 'race_recode_5',\n 'hispanic_origin',\n 'hispanic_originrace_recode']"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns of MergeData\n",
    "MergeData.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Dropping Columns\n",
    "MergeData = MergeData.drop('record_condition_1',\n",
    "                           'record_condition_2',\n",
    "                           'record_condition_3',\n",
    "                           'record_condition_4',\n",
    "                           'record_condition_5',\n",
    "                           'record_condition_6',\n",
    "                           'record_condition_7',\n",
    "                           'record_condition_8',\n",
    "                           'record_condition_9',\n",
    "                           'record_condition_10',\n",
    "                           'record_condition_11',\n",
    "                           'record_condition_12',\n",
    "                           'record_condition_13',\n",
    "                           'record_condition_14',\n",
    "                           'record_condition_15',\n",
    "                           'record_condition_16',\n",
    "                           'record_condition_17',\n",
    "                           'record_condition_18',\n",
    "                           'record_condition_19',\n",
    "                           'record_condition_20')\n",
    "\n",
    "MergeData = MergeData.drop('113_cause_recode',\n",
    "                           '130_infant_cause_recode',\n",
    "                           '39_cause_recode',\n",
    "                           'number_of_entity_axis_conditions',\n",
    "                           'entity_condition_1',\n",
    "                           'entity_condition_2',\n",
    "                           'entity_condition_3',\n",
    "                           'entity_condition_4',\n",
    "                           'entity_condition_5',\n",
    "                           'entity_condition_6',\n",
    "                           'entity_condition_7',\n",
    "                           'entity_condition_8',\n",
    "                           'entity_condition_9',\n",
    "                           'entity_condition_10',\n",
    "                           'entity_condition_11',\n",
    "                           'entity_condition_12',\n",
    "                           'entity_condition_13',\n",
    "                           'entity_condition_14',\n",
    "                           'entity_condition_15',\n",
    "                           'entity_condition_16',\n",
    "                           'entity_condition_17',\n",
    "                           'entity_condition_18',\n",
    "                           'entity_condition_19')\n",
    "\n",
    "MergeData = MergeData.drop('icd_code_10th_revision',\n",
    "                           'age_recode_27',\n",
    "                           'age_recode_12',\n",
    "                           'detail_age',\n",
    "                           'entity_condition_20',\n",
    "                           'education_2003_revision',\n",
    "                           'education_1989_revision')\n",
    "\n",
    "MergeData = MergeData.filter((MergeData.method_of_disposition == 'B') | (MergeData.method_of_disposition == 'C' ))\n",
    "\n",
    "MergeData = MergeData.drop('detail_age_type',\n",
    "                           'age_substitution_flag',\n",
    "                           'age_substitution_flag',\n",
    "                           'infant_age_recode_22',\n",
    "                           'day_of_week_of_death',\n",
    "                           'current_data_year',\n",
    "                           '358_cause_recode',\n",
    "                           'number_of_record_axis_conditions',\n",
    "                           'hispanic_origin',\n",
    "                           'race_recode_5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "['resident_status',\n 'education_reporting_flag',\n 'month_of_death',\n 'sex',\n 'age_recode_52',\n 'place_of_death_and_decedents_status',\n 'marital_status',\n 'injury_at_work',\n 'manner_of_death',\n 'method_of_disposition',\n 'autopsy',\n 'activity_code',\n 'place_of_injury_for_causes_w00_y34_except_y06_and_y07_',\n 'race',\n 'bridged_race_flag',\n 'race_imputation_flag',\n 'race_recode_3',\n 'hispanic_originrace_recode']"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns of MergeData\n",
    "MergeData.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "categoricalColumns = ['resident_status',\n",
    "                      'month_of_death',\n",
    "                      'sex',\n",
    "                      'age_recode_52',\n",
    "                      'place_of_death_and_decedents_status',\n",
    "                      'marital_status',\n",
    "                      'injury_at_work',\n",
    "                      'manner_of_death',\n",
    "                      'autopsy',\n",
    "                      'activity_code',\n",
    "                      'place_of_injury_for_causes_w00_y34_except_y06_and_y07_',\n",
    "                      'race',\n",
    "                      'race_recode_3',\n",
    "                      'hispanic_originrace_recode']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Null Imputations\n",
    "MergeData = MergeData.fillna({'place_of_injury_for_causes_w00_y34_except_y06_and_y07_': 12})\n",
    "MergeData = MergeData.fillna({'activity_code': 11})\n",
    "MergeData = MergeData.fillna({'manner_of_death': 999})\n",
    "MergeData = MergeData.fillna({'place_of_death_and_decedents_status': 999})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "DataFrame[resident_status: int, education_1989_revision: int, education_2003_revision: int, education_reporting_flag: int, month_of_death: int, sex: string, detail_age_type: int, detail_age: int, age_substitution_flag: int, age_recode_52: int, age_recode_27: int, age_recode_12: int, infant_age_recode_22: int, place_of_death_and_decedents_status: int, marital_status: string, day_of_week_of_death: int, current_data_year: int, injury_at_work: string, manner_of_death: int, method_of_disposition: string, autopsy: string, activity_code: int, place_of_injury_for_causes_w00_y34_except_y06_and_y07_: int, icd_code_10th_revision: string, 358_cause_recode: int, 113_cause_recode: int, 130_infant_cause_recode: int, 39_cause_recode: int, number_of_entity_axis_conditions: int, entity_condition_1: string, entity_condition_2: string, entity_condition_3: string, entity_condition_4: string, entity_condition_5: string, entity_condition_6: string, entity_condition_7: string, entity_condition_8: string, entity_condition_9: string, entity_condition_10: string, entity_condition_11: string, entity_condition_12: string, entity_condition_13: string, entity_condition_14: string, entity_condition_15: string, entity_condition_16: string, entity_condition_17: string, entity_condition_18: string, entity_condition_19: string, entity_condition_20: string, number_of_record_axis_conditions: int, record_condition_1: string, record_condition_2: string, record_condition_3: string, record_condition_4: string, record_condition_5: string, record_condition_6: string, record_condition_7: string, record_condition_8: string, record_condition_9: string, record_condition_10: string, record_condition_11: string, record_condition_12: string, record_condition_13: string, record_condition_14: string, record_condition_15: string, record_condition_16: string, record_condition_17: string, record_condition_18: string, record_condition_19: string, record_condition_20: string, race: int, bridged_race_flag: int, race_imputation_flag: int, race_recode_3: int, race_recode_5: int, hispanic_origin: int, hispanic_originrace_recode: int]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merging all years data into dataframe\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "JoinedData=unionAll(year2005,\n",
    "                    year2006,\n",
    "                    year2007,\n",
    "                    year2008,\n",
    "                    year2009,\n",
    "                    year2010,\n",
    "                    year2011,\n",
    "                    year2012,\n",
    "                    year2013,\n",
    "                    year2014,\n",
    "                    year2015)\n",
    "\n",
    "display(JoinedData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Replacing null values in manner_of_death\n",
    "JoinData = JoinedData.fillna({'manner_of_death': 12})\n",
    "JoinData = JoinedData.filter(JoinedData.manner_of_death == '2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Creating joint table on new joined data which is modified\n",
    "JoinData.registerTempTable(\"jointTable\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+\n",
      "|year|suicides|counts|\n",
      "+----+--------+------+\n",
      "|2005| Suicide| 32934|\n",
      "|2006| Suicide| 33562|\n",
      "|2007| Suicide| 34827|\n",
      "|2008| Suicide| 36251|\n",
      "|2009| Suicide| 37205|\n",
      "|2010| Suicide| 38710|\n",
      "|2011| Suicide| 39878|\n",
      "|2012| Suicide| 40929|\n",
      "|2013| Suicide| 41509|\n",
      "|2014| Suicide| 43139|\n",
      "|2015| Suicide| 44417|\n",
      "+----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 8 - Total suicides committed 2005-2015\n",
    "results_suicides = spark.sql(\n",
    "  \"\"\"SELECT current_data_year AS year,\n",
    "            CASE manner_of_death\n",
    "            WHEN '2' THEN 'Suicide'\n",
    "            END AS suicides,\n",
    "            COUNT(*) AS counts\n",
    "     FROM jointTable\n",
    "     GROUP BY 1, 2\n",
    "     ORDER BY 1, 3\"\"\")\n",
    "\n",
    "results_suicides.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_suicides (ind int primary key, counts int, suicides varchar, year int)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_suicides (ind, counts, suicides, year) VALUES (?, ?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_suicides.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['counts'], item['suicides'], item['year']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|         description|counts|\n",
      "+--------------------+------+\n",
      "|Ill-defined and u...| 42543|\n",
      "|Acute myocardial ...|   294|\n",
      "|      Cardiac arrest|   214|\n",
      "|Hypertensive hear...|   195|\n",
      "|Complications and...|   166|\n",
      "|Other chronic obs...|   143|\n",
      "|      Cardiomyopathy|   117|\n",
      "|Cardiac arrhythmi...|    92|\n",
      "|Pneumonia, unspec...|    89|\n",
      "|Essential (primar...|    75|\n",
      "|Sepsis, unspecifi...|    75|\n",
      "|Shock, not elsewh...|    58|\n",
      "|Other disorders o...|    54|\n",
      "|       Other obesity|    51|\n",
      "|Chronic ischemic ...|    49|\n",
      "| Alzheimer's disease|    42|\n",
      "|Obesity, unspecified|    38|\n",
      "|           Emphysema|    36|\n",
      "|Nontraumatic intr...|    33|\n",
      "|  Respiratory arrest|    32|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 9 - Result about deaths pending\n",
    "results_deaths_pending = spark.sql(\n",
    "    \"\"\"SELECT icd.description3 AS description,\n",
    "              count(*) as counts\n",
    "       FROM mergedTable mt JOIN icd10 icd ON icd.code3 = mt.icd_code_10th_revision\n",
    "       WHERE mt.manner_of_death == '4'\n",
    "       GROUP BY description\n",
    "       ORDER BY counts DESC, description\"\"\"\n",
    ")\n",
    "results_deaths_pending.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_deaths_pending (ind int primary key, counts int, description varchar)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_deaths_pending (ind, counts, description) VALUES (?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_deaths_pending.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['counts'], item['description']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "| counts|      day|\n",
      "+-------+---------+\n",
      "|   1196|  Unknown|\n",
      "|3931885|  Tuesday|\n",
      "|3933510|   Sunday|\n",
      "|3939328|Wednesday|\n",
      "|3945930| Thursday|\n",
      "|3960250|   Monday|\n",
      "|3996471|   Friday|\n",
      "|4012103| Saturday|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 10 - Deaths for days of the week\n",
    "results_day_week = spark.sql(\n",
    "    \"\"\"SELECT count(*) as counts,\n",
    "              CASE day_of_week_of_death\n",
    "              WHEN '1' THEN 'Sunday'\n",
    "              WHEN '2' THEN 'Monday'\n",
    "              WHEN '3' THEN 'Tuesday'\n",
    "              WHEN '4' THEN 'Wednesday'\n",
    "              WHEN '5' THEN 'Thursday'\n",
    "              WHEN '6' THEN 'Friday'\n",
    "              WHEN '7' THEN 'Saturday'\n",
    "              WHEN '9' THEN 'Unknown'\n",
    "              END AS day\n",
    "      FROM mergedTable\n",
    "      GROUP BY day\n",
    "      ORDER BY counts\"\"\"\n",
    ")\n",
    "\n",
    "results_day_week.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_day_week (ind int primary key, counts int, day varchar)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_day_week (ind, counts, day) VALUES (?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_day_week.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['counts'], item['day']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|  counts|          skin_color|\n",
      "+--------+--------------------+\n",
      "|  759081|Races other than b&w|\n",
      "| 3258257|               Black|\n",
      "|23703335|               White|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 11 - Total deaths by skin's color\n",
    "results_deaths_skin_color = spark.sql(\n",
    "    \"\"\"SELECT count(*) as counts,\n",
    "            CASE race_recode_3\n",
    "            WHEN '1' THEN 'White'\n",
    "            WHEN '2' THEN 'Races other than b&w'\n",
    "            WHEN '3' THEN 'Black'\n",
    "            END AS skin_color\n",
    "       FROM mergedTable\n",
    "       GROUP BY skin_color\n",
    "       ORDER BY counts\"\"\"\n",
    ")\n",
    "\n",
    "results_deaths_skin_color.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Saving data in the db\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS results_deaths_skin_color (ind int primary key, counts int, skin_color varchar)\")\n",
    "stmt = session.prepare(\"INSERT INTO results_deaths_skin_color (ind, counts, skin_color) VALUES (?, ?, ?)\")\n",
    "\n",
    "for ind, item in results_deaths_skin_color.toPandas().iterrows():\n",
    "    results = session.execute(stmt, [ind, item['counts'], item['skin_color']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Spark MLlib\n",
    "\n",
    "# Pipeline- String Indexing, encoding and Vector Assembling\n",
    "stages = [] # stages in our Pipeline\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol='method_of_disposition', outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns]\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(MergeData)\n",
    "preppedDataDF = pipelineModel.transform(MergeData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Choose the column that is predicted and the columns(features) that are used for prediciting\n",
    "selectedcols = [\"label\", \"features\"]\n",
    "dataset = preppedDataDF.select(selectedcols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Splitting the dataset into training set and test set\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Logistic Regression model for predicting method-of-disposition (burial vs cremation)\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(trainingData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Comparing the predicted value against actual value\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6650950346224196"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Efficiency of Model: Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "0.6298288458452562"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy: Evaluate model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", metricName = 'accuracy')\n",
    "evaluator.evaluate(predictions)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}